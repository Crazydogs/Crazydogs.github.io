---
layout: post
title:  "Python 机器学习 - 预测分析核心算法(5)"
date:   2017-03-23 20:44:10
category: coding
---

## 第六章 集成方法

### 集成方法的原理
如果模型之间近似相互独立，那么多个模型联合的性能要优于单个模型。
如果一个分类器有 55% 的概率给出正确结果，那一百个这样的分类器联合正确率可以上升到 82% （具体可以看『累积二项式概率分布』）

### 获取多个模型的方法
不同的机器学习算法可以产生不同的模型，如支持向量机(SVM)，线性回归，k最临近、二元决策数等。
但靠不同的算法难以产生大量的模型，不适应于须要成百上千模型的场景

### 集成方法
集成方法是由两层算法组成的层次架构。
底层的算法叫做基学习器，基学习器是单个机器学习算法，本章主要使用二元决策树作为基学习器
上层算法通过对这些基学习器进行处理，使其模型近似相对独立。
广泛使用的上层算法有：投票(bagging)、提升(boosting)和随机森林(random forests)。
严格地说，随机森林实际上是上层算法和修改后的二元决策树的组合。
有许多算法可以作为基学习器，如二元决策树、支持向量机等。但从实用角度二元决策树应用最为广泛。

### 二元决策树
基于属性做一系列的二元（是/否）决策，每次决策对应于从两种可能性中选择一个。
每次决策之后，引出另一个决策或者生成最终结果。

#### 分割点选择算法

1.搜索所有可能的分割点，对每个点执行
    1.将其作为分割点，将数据分成两部分
    2.对每一部分求平均值作为预测值，然后求出预测的误差。
2.找出误差最小的分割点
        
#### 多变量决策树的训练
如果问题含有多个属性，则须要对所有属性检查所有可能的分割点。
随着数据规模的增大，分割点的计算量也成比例增加。所以实际应用中，
分割点检测通常比原始数据粒度粗很多

#### 通过递归分割获得更深的决策树
决策树深度的增加意味着更小的步长和更高的准确性。
如果深度过深，导致决策树节点只有一个数据实例，就不用再进行分割了。
如果节点包含的数据实例太少，会导致预测结果发生剧烈的震荡。

#### 二元决策树的过拟合
过拟合问题可以通过决策树的参数（深度、最小叶节点规模）
在实际问题中，可以使用较差验证(cross-validation)来控制过拟合。
针对不同的决策树深度进行交叉验证，以获取在测试集上误差较小的深度。
如果数据量增大，通畅最优深度也会变大。更多的数据支持更复杂的模型。

#### 分类问题和类别特征

1.针对分类问题，和回归问题相比，其评价标准不同。不使用均方差而是使用误分类率、
基尼不纯性度量、信息增益等指标。
2.针对类别属性，分割子集的时候须要尝试所有分隔可能，当类别多的时候，分隔可能会非常多。

### 集成算法
二元决策树是一个很好的预测工具，但单个的基学习器还是有很多问题（如要调整多个参数，结果不稳定、容易过拟合等），这些可以通过集成方法来减弱。

#### 自举集成 Bagging 算法
采用『自举』采样方法，从数据集中获取取样。自举取样是放回式地随机选择元素。
从训练数据集中获得一系列自举样本之后，对每个样本训练一个基学习器。
对于回归问题，结果为基学习器的均值，对于分类问题，结果是从不同类别所占百分比引申出来的各种类别的概率或均值。

#### 梯度提升树模型
梯度提升树模型的每一次迭代都以上一轮迭代训练的残差作为训练目标。
基本步骤如下
1. 初始化残差为标签值(即第一次迭代还是以 y 为标签进行训练，也就是说原始的预测值为 0)
对于第 i 轮迭代
    1. 使用训练集的属性值和上一轮的残差训练基学习器，得到模型 fi
    2. 使用模型 fi 对训练集进行预测，得到预测值 prei
    3. 将残差更新为 （旧残差 - eps * prei）其中 eps 为梯度提升步长
2. 使用多轮的模型求和得到最终结果

具体代码可以参照 [这里](https://github.com/Crazydogs/python_machine_learning_example/blob/master/gradientBoosting.py)


